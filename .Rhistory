model_results=compute(concrete_model,concrete_test[1:8])
# it returns a list with two components: neurons, which stores the neurons for each layer in the network, and net.result, which stores the predicted values
predicted_strength<-model_results$net.result
cor(predicted_strength,concrete_test$strength)
# load the dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//social.csv")
str(data)
# load the dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//social.csv")
str(data)
data<-data[3:5]  # the first column is userid, which doesn't contribute for the classification
# encoding the target feature as a factor
data$Purchased<-factor(data$Purchased,levels = c(0,1))
# splitting the dataset into training and testing
library(caTools)
set.seed(123)
split<-sample.split(data$Purchased,SplitRatio = 0.75)
training_set<-subset(data,split==TRUE)
testing_Set<-subset(data,split==FALSE)
# feature scaling
# Scaling is used to change the data within the range of data
training_set[-3]<-scale(training_set[-3])  # not considering the 3rd column of the data for scaling because it is the target feature
testing_Set[-3]<-scale(testing_Set[-3])
library(e1071)
classifier<-svm(formula=Purchased~.,
data=training_set,
type='C-classification',
kernel='linear'
)
# kernel is used to specify the type of hyperplane to be plotted.
# Predicting the test set results
y_pred<-predict(classifier,newdata = testing_Set[-3])   # not considering the 3rd column because that is the feature needed to be predicted
cm<-table(testing_Set[,3],y_pred)
cm
print(paste("Accuracy of the model:",(sum(diag(cm))/sum(cm))*100))
# loading the packages
library(arules)
library(cluster)
# removing the initial label of the dataset i.e. name of species since this is unsupervised learning
data<-iris[,-5]
# fitting k-means clusering model to the training dataset
set.seed(240) #setting seed
kmeans.re<-kmeans(data,centers=3,nstart = 20)
# centers = 3: This specifies that you want 3 clusters, so the algorithm will aim to divide your data into 3 groups.
# nstart = 20: This indicates that K-means should run 20 different initializations and select the one with the best clustering result (i.e., the one with the lowest sum of squared distances within clusters). Running multiple initializations is a good practice to avoid poor clustering results that can occur from random initialization of centroids.
kmeans.re
# cluster identification for each observation
kmeans.re$cluster
kmeans.re$centers
# confusion matrix
cm<-table(iris$Species,kmeans.re$cluster)
cm
print(paste("Accuracy: ",sum(diag(cm))/sum(cm)*100,"%"))
# model evaluation and visualization
plot(data[c("Sepal.Length","Sepal.Width")],
col=kmeans.re$cluster,
main="K-means clustering with 3 clusters")
# plotting cluster centers
kmeans.re$centers
kmeans.re$centers[,c("Sepal.Length","Sepal.Width")]
# cex is font size and pch is symbol
points(kmeans.re$centers[,c("Sepal.Length","Sepal.Width")],
col=1:3,pch=8,cex=3)
# visualizing clusters
y_kmeans<-kmeans.re$cluster
clusplot(data[,c("Sepal.Length","Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels=2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab='Sepal.Length',
ylab='Sepal.Width')
# lines=0 means no distance lines between the elipses will be there
# loading the packages
library(arules)
library(cluster)
# removing the initial label of the dataset i.e. name of species since this is unsupervised learning
data<-iris[,-5]
# fitting k-means clusering model to the training dataset
set.seed(240) #setting seed
kmeans.re<-kmeans(data,centers=3,nstart = 20)
# centers = 3: This specifies that you want 3 clusters, so the algorithm will aim to divide your data into 3 groups.
# nstart = 20: This indicates that K-means should run 20 different initializations and select the one with the best clustering result (i.e., the one with the lowest sum of squared distances within clusters). Running multiple initializations is a good practice to avoid poor clustering results that can occur from random initialization of centroids.
kmeans.re
# cluster identification for each observation
kmeans.re$cluster
kmeans.re$centers
# confusion matrix
cm<-table(iris$Species,kmeans.re$cluster)
cm
print(paste("Accuracy: ",sum(diag(cm))/sum(cm)*100,"%"))
# model evaluation and visualization
plot(data[c("Sepal.Length","Sepal.Width")],
col=kmeans.re$cluster,
main="K-means clustering with 3 clusters")
# plotting cluster centers
kmeans.re$centers
kmeans.re$centers[,c("Sepal.Length","Sepal.Width")]
# cex is font size and pch is symbol
points(kmeans.re$centers[,c("Sepal.Length","Sepal.Width")],
col=1:3,pch=8,cex=3)
# visualizing clusters
y_kmeans<-kmeans.re$cluster
clusplot(data[,c("Sepal.Length","Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels=2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab='Sepal.Length',
ylab='Sepal.Width')
# loading the packages
library(arules)
library(cluster)
# removing the initial label of the dataset i.e. name of species since this is unsupervised learning
data<-iris[,-5]
# fitting k-means clusering model to the training dataset
set.seed(240) #setting seed
kmeans.re<-kmeans(data,centers=3,nstart = 20)
# centers = 3: This specifies that you want 3 clusters, so the algorithm will aim to divide your data into 3 groups.
# nstart = 20: This indicates that K-means should run 20 different initializations and select the one with the best clustering result (i.e., the one with the lowest sum of squared distances within clusters). Running multiple initializations is a good practice to avoid poor clustering results that can occur from random initialization of centroids.
kmeans.re
# cluster identification for each observation
kmeans.re$cluster
kmeans.re$centers
# confusion matrix
cm<-table(iris$Species,kmeans.re$cluster)
cm
print(paste("Accuracy: ",sum(diag(cm))/sum(cm)*100,"%"))
# loading the packages
library(arules)
library(cluster)
# removing the initial label of the dataset i.e. name of species since this is unsupervised learning
data<-iris[,-5]
# fitting k-means clusering model to the training dataset
set.seed(240) #setting seed
kmeans.re<-kmeans(data,centers=2,nstart = 20)
# centers = 3: This specifies that you want 3 clusters, so the algorithm will aim to divide your data into 3 groups.
# nstart = 20: This indicates that K-means should run 20 different initializations and select the one with the best clustering result (i.e., the one with the lowest sum of squared distances within clusters). Running multiple initializations is a good practice to avoid poor clustering results that can occur from random initialization of centroids.
kmeans.re
# cluster identification for each observation
kmeans.re$cluster
kmeans.re$centers
# confusion matrix
cm<-table(iris$Species,kmeans.re$cluster)
cm
print(paste("Accuracy: ",sum(diag(cm))/sum(cm)*100,"%"))
kmeans.re<-kmeans(data,centers=4,nstart = 20)
# centers = 3: This specifies that you want 3 clusters, so the algorithm will aim to divide your data into 3 groups.
# nstart = 20: This indicates that K-means should run 20 different initializations and select the one with the best clustering result (i.e., the one with the lowest sum of squared distances within clusters). Running multiple initializations is a good practice to avoid poor clustering results that can occur from random initialization of centroids.
kmeans.re
# cluster identification for each observation
kmeans.re$cluster
kmeans.re$centers
# confusion matrix
cm<-table(iris$Species,kmeans.re$cluster)
cm
print(paste("Accuracy: ",sum(diag(cm))/sum(cm)*100,"%"))
# model evaluation and visualization
plot(data[c("Sepal.Length","Sepal.Width")],
col=kmeans.re$cluster,
main="K-means clustering with 3 clusters")
# plotting cluster centers
kmeans.re$centers
kmeans.re$centers[,c("Sepal.Length","Sepal.Width")]
# cex is font size and pch is symbol
points(kmeans.re$centers[,c("Sepal.Length","Sepal.Width")],
col=1:3,pch=8,cex=3)
# visualizing clusters
y_kmeans<-kmeans.re$cluster
clusplot(data[,c("Sepal.Length","Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels=2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab='Sepal.Length',
ylab='Sepal.Width')
# loading the packages
library(arules)
library(cluster)
# removing the initial label of the dataset i.e. name of species since this is unsupervised learning
data<-iris[,-5]
# fitting k-means clusering model to the training dataset
set.seed(240) #setting seed
kmeans.re<-kmeans(data,centers=3,nstart = 20)
# centers = 3: This specifies that you want 3 clusters, so the algorithm will aim to divide your data into 3 groups.
# nstart = 20: This indicates that K-means should run 20 different initializations and select the one with the best clustering result (i.e., the one with the lowest sum of squared distances within clusters). Running multiple initializations is a good practice to avoid poor clustering results that can occur from random initialization of centroids.
kmeans.re
# cluster identification for each observation
kmeans.re$cluster
kmeans.re$centers
# confusion matrix
cm<-table(iris$Species,kmeans.re$cluster)
cm
print(paste("Accuracy: ",sum(diag(cm))/sum(cm)*100,"%"))
# model evaluation and visualization
plot(data[c("Sepal.Length","Sepal.Width")],
col=kmeans.re$cluster,
main="K-means clustering with 3 clusters")
# plotting cluster centers
kmeans.re$centers
kmeans.re$centers[,c("Sepal.Length","Sepal.Width")]
# cex is font size and pch is symbol
points(kmeans.re$centers[,c("Sepal.Length","Sepal.Width")],
col=1:3,pch=8,cex=3)
# visualizing clusters
y_kmeans<-kmeans.re$cluster
clusplot(data[,c("Sepal.Length","Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels=2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab='Sepal.Length',
ylab='Sepal.Width')
?cluster
# model evaluation and visualization
plot(data[c("Sepal.Length","Sepal.Width")],
col=kmeans.re$cluster,
main="K-means clustering with 3 clusters")
# plotting cluster centers
kmeans.re$centers
kmeans.re$centers[,c("Sepal.Length","Sepal.Width")]
# cex is font size and pch is symbol
points(kmeans.re$centers[,c("Sepal.Length","Sepal.Width")],
col=1:3,pch=8,cex=3)
# visualizing clusters
y_kmeans<-kmeans.re$cluster
clusplot(data[,c("Sepal.Length","Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels=2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab='Sepal.Length',
ylab='Sepal.Width')
# model evaluation and visualization
plot(data[c("Sepal.Length","Sepal.Width")],
col=kmeans.re$cluster,
main="K-means clustering with 3 clusters")
# plotting cluster centers
kmeans.re$centers
kmeans.re$centers[,c("Sepal.Length","Sepal.Width")]
# cex is font size and pch is symbol
points(kmeans.re$centers[,c("Sepal.Length","Sepal.Width")],
col=1:3,pch=8,cex=3)
clusplot(data[,c("Sepal.Length","Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels=2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab='Sepal.Length',
ylab='Sepal.Width')
iris
iris1<-iris
iris1
iris1$Species=NULL
iris
iris1<-iris
iris1
iris1$Species=NULL
d=dist(iris1,method="euclidean")
hfit=hclust(d)
plot(hfit)
grps=cutree(hfit,k=4)
grps
rect.hclust(hfit,k=4,border = "red")
library("arules")
data("Groceries")   # this dataset is the part of the library arules
View(Groceries)
summary(Groceries)
str(Groceries)
# Look for first five items
inspect(Groceries[1:5])
#examine the frequency of items
itemFrequency(Groceries[,1:3])
# plot the frequency of items
itemFrequencyPlot(Groceries)
itemFrequencyPlot(Groceries, support=0.1)
itemFrequencyPlot(Groceries, topN=20)
# a visualization of the sparse matrix for the first five transactions
image(Groceries[1:5])
# visualization of a random sample of 100 transactions
image(sample(Groceries,100))
# training the model on data
# set better support and confidence levels to learn more rules
groceryrules<-apriori(Groceries,
parameter = list(support=0.006, confidence=0.25,minlen=2))
groceryrules
# evaluating the model performance ------- summary of grocery association rules
summary(groceryrules)
# look at the first three rules
inspect(groceryrules[1:3])
# writing the rules into a csv file
write(groceryrules, file = "C:/lpu/5th sem/INT234/Code/groceryrules.csv", sep=",",
quote=TRUE, row.names=FALSE)
# look at the first three rules
inspect(groceryrules[1:3])
# improving the model performance
inspect(groceryrules[1:10],lift=0.5)
# sorting grocery rules by lift
inspect(sort(groceryrules,by="lift")[1:5])
# finding the subsets of rules containing any berry items
berryrules<-subset(groceryrules,items%in%"berries")
inspect(berryrules)
# finding the subsets of rules that precede soda purchases
sodarules<-subset(groceryrules,rhs %in% "soda")
inspect(sodarules)
top.soda.rules<-head(sort(sodarules, by="lift"),5)
inspect(top.soda.rules)
# writing the rules into a csv file
write(groceryrules, file = "C:/lpu/5th sem/INT234/Code/groceryrules.csv", sep=",",
quote=TRUE, row.names=FALSE)
getwd()
# converting the rule set into a dataframe
groceryrules_df<-as(groceryrules,"data.frame")
str(groceryrules_df)
Groceries
# loading the necessary packages
library(randomForest)
# load the dataset
data("iris")
set.seed(42)
sample_index<-sample(1:nrow(iris),0.7*nrow(iris))
train_data<-iris[sample_index,]
test_data<-iris[-sample_index,]
# train the model
rf_model<-randomForest(Species ~ ., data = train_data, ntree=10, mtry=2)
# model summary
rf_model
# predict the test set
predictions<-predict(rf_model,test_data)
# confusion matrix to evaluate the model
confusion_matrix<-table(predictions, test_data$Species)
confusion_matrix
print(sum(diag(confusion_matrix))/sum(confusion_matrix)*100)
# model summary
plot(rf_model)
# load the dataset
data("iris")
set.seed(42)
sample_index<-sample(1:nrow(iris),0.7*nrow(iris))
train_data<-iris[sample_index,]
test_data<-iris[-sample_index,]
# train the model
rf_model<-randomForest(Species ~ ., data = train_data, ntree=10, mtry=2)
# model summary
rf_model
# predict the test set
predictions<-predict(rf_model,test_data)
# confusion matrix to evaluate the model
confusion_matrix<-table(predictions, test_data$Species)
confusion_matrix
print(sum(diag(confusion_matrix))/sum(confusion_matrix)*100)
library("arules")
data("Groceries")   # this dataset is the part of the library arules
summary(Groceries)
str(Groceries)
# Look for first five items
inspect(Groceries[1:5])
#examine the frequency of items
itemFrequency(Groceries[,1:3])
# plot the frequency of items
itemFrequencyPlot(Groceries)
itemFrequencyPlot(Groceries, support=0.1)
itemFrequencyPlot(Groceries, topN=20)
# a visualization of the sparse matrix for the first five transactions
image(Groceries[1:5])
# visualization of a random sample of 100 transactions
image(sample(Groceries,100))
# training the model on data
# set better support and confidence levels to learn more rules
groceryrules<-apriori(Groceries,
parameter = list(support=0.006, confidence=0.25,minlen=2))
groceryrules
# evaluating the model performance ------- summary of grocery association rules
summary(groceryrules)
# look at the first three rules
inspect(groceryrules[1:3])
# improving the model performance
inspect(groceryrules[1:10],lift=0.5)
# sorting grocery rules by lift
inspect(sort(groceryrules,by="lift")[1:5])
# finding the subsets of rules containing any berry items
berryrules<-subset(groceryrules,items%in%"berries")
inspect(berryrules)
# finding the subsets of rules that precede soda purchases
sodarules<-subset(groceryrules,rhs %in% "soda")
inspect(sodarules)
top.soda.rules<-head(sort(sodarules, by="lift"),5)
inspect(top.soda.rules)
# converting the rule set into a dataframe
groceryrules_df<-as(groceryrules,"data.frame")
str(groceryrules_df)
library("arules")
data("Groceries")   # this dataset is the part of the library arules
summary(Groceries)
str(Groceries)
# Look for first five items
inspect(Groceries[1:5])
#examine the frequency of items
itemFrequency(Groceries[,1:3])
# plot the frequency of items
itemFrequencyPlot(Groceries)
itemFrequencyPlot(Groceries, support=0.1)
itemFrequencyPlot(Groceries, topN=20)
# a visualization of the sparse matrix for the first five transactions
image(Groceries[1:5])
# visualization of a random sample of 100 transactions
image(sample(Groceries,100))
# training the model on data
# set better support and confidence levels to learn more rules
groceryrules<-apriori(Groceries,
parameter = list(support=0.006, confidence=0.25,minlen=2))
groceryrules
# sorting grocery rules by lift
inspect(sort(groceryrules,by="lift")[1:5])
# decision trees
# step-1: load the necessary packages
library(rpart)
library(rpart.plot)
# step-2: load the iris dataset
data(iris)
str(iris)
# step-3: split the data into training and testing sets
set.seed(42)  # setting seed for reproducibility
indexes=sample(1:nrow(iris),0.7*nrow(iris))  # randomly selecting the indexes from iris dataset for training and testing
# as we are randomising the data for training and testing, the data will be randomised everytime we run the code, so we need to use set.seed so that the same data will be used which was previously randomised
iris_train<-iris[indexes,]  # training data(70%)
View(iris_train)
iris_test<-iris[-indexes,] # testing data(30%)
# step-4: define the target formula and train the decision tree
target<-Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width  # we are saying that the target/ feature to be classified is Species by comupting the columns given after '~' sign
# tree=rpart(target, data=iris_train, method="class, control=rpart.control(minsplit=4))
tree<-rpart(target, data=iris_train, method="class") #class defines that the tree should be made for classification, not for regression
# step-5: visualize the decision tree
rpart.plot(tree)
# step-6: make predictions on the test set
predictions<-predict(tree,iris_test,type="class")
# step-7: evaluate the model performance by creating a confusion matrix
confusion_matrix<-table(iris_test$Species,predictions)
print(confusion_matrix)
# step-8: calculate the accuracy of the model
accuracy=sum(diag(confusion_matrix))/sum(confusion_matrix)  # the formula used is present in 'caret' library. this calculates the sum of diagonal values present in the confusion matrix divided by sum of all values present in confusion matrix
print(paste("Accuracy: ",round(accuracy*100,4)))
# linear regression: one independent and one dependent variables
# applying linear regression model on iris dataset
data(iris)
head(iris)
# training the linear regression model using lm() function
model<-lm(Sepal.Length~Petal.Length, data = iris)
summary(model)
# plotting the regression chart
plot(iris$Petal.Length,iris$Sepal.Length, main = "Linear Regression on iris data",xlab="Petal Length",ylab="Sepal Length",pch=19,col="blue")
# adding the regression line to the previous chart
abline(model,col="red")
insurance<-read.csv("C://lpu//5th sem//INT234//Datasets//insurance.csv",stringsAsFactors = FALSE)
str(insurance)
summary(insurance$charges)
hist(insurance$charges)  # generates a histogram for the given data values
table(insurance$region)  #builds a contingency table of the counts at each combination of factor level
cor(insurance[c("age","bmi","children","charges")])  # cor is used to find the correlation between the columns of x and y
# SVM: Support Vector Machine
# load the dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//social.csv")
str(data)
data<-data[3:5]  # the first column is userid, which doesn't contribute for the classification
# encoding the target feature as a factor
data$Purchased<-factor(data$Purchased,levels = c(0,1))
# splitting the dataset into training and testing
library(caTools)
set.seed(123)
split<-sample.split(data$Purchased,SplitRatio = 0.75)
training_set<-subset(data,split==TRUE)
testing_Set<-subset(data,split==FALSE)
# feature scaling
# Scaling is used to change the data within the range of data
training_set[-3]<-scale(training_set[-3])  # not considering the 3rd column of the data for scaling because it is the target feature
testing_Set[-3]<-scale(testing_Set[-3])
library(e1071)
classifier<-svm(formula=Purchased~.,
data=training_set,
type='C-classification',
kernel='linear'
)
# kernel is used to specify the type of hyperplane to be plotted.
# Predicting the test set results
y_pred<-predict(classifier,newdata = testing_Set[-3])   # not considering the 3rd column because that is the feature needed to be predicted
cm<-table(testing_Set[,3],y_pred)
cm
print(paste("Accuracy of the model:",(sum(diag(cm))/sum(cm))*100))
