irisd
table(irisd$Species)
irisd$Species<-factor(irisd$Species,levels=c("setosa","versicolor","virginica"),labels = c("Setosa","Versicolor","Virginica"))
round(prop.table(table(irisd$Species))*100,digits = 1)
summary(irisd[c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
irisd_n<-as.data.frame(lapply(irisd[1:4],normalize))
summary(irisd_n)
str(irisd_n)
irisd_train<-irisd_n[1:100,]
irisd_test<-irisd_n[101:150,]
irisd_train_labels<-irisd[1:100,5]
irisd_test_labels<-irisd[101:150,5]
library(class)
irisd_test_prepd<-knn(train = irisd_train,test = irisd_test,cl=irisd_train_labels,k=10)
library(gmodels)
CrossTable(x=irisd_test_labels,y=irisd_test_prepd,prop.chisq=FALSE)
iris_raw<-iris
View(iris_raw)
iris_raw<-iris
iris_raw<-iris
str(iris_raw)
# not converting the species into factos as it was already in factor
# no need of data mining and data cleaning
iris_train<-iris_raw[1:75,]
iris_test<-iris_raw[76:150,]
iris_train_labels<-iris_raw[1:75,]$Species
iris_test_labels<-iris_raw[76:150,]$Species
library(e1071)
iris_classifier<-naiveBayes(iris_train,iris_train_labels)
iris_test_pred<-predict(iris_classifier,iris_test)
a=table(iris_test_pred,iris_test_labels)
a
library(gmodels)
CrossTable(iris_test_pred,iris_test_labels,prop.chisq = FALSE,prop.t = FALSE,dnn = c('predicted','actual'))  # making a table which tells about how many values have been predicted correctly classified by predicted and actual
library(caret)
confusionMatrix(a)
# naive bayes on intrusion detection
data<-read.csv(file.choose())
data
View(data)
str(data)
# got to know that class can be a factor
data$class<-factor(data$class)
str(data)
library(tm)
nrow(data)
ncol(data)
data_train<-data[1:16544,]
data_test<-data[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(e1071)
data_classifier<-naiveBayes(data_train,data_train_labels)
data_pred<-predict(data_classifier,data_test)
a=table(data_pred,data_test_labels)
a
library(gmodels)
CrossTable(data_pred,data_test_labels,prop.chisq = FALSE,prop.t = FALSE,dnn = c('predicted','actual'))
library(caret)
confusionMatrix(a)
gc()
# naive bayes on intrusion detection
data<-read.csv(file.choose())
data
str(data)
# got to know that class can be a factor
data$class<-factor(data$class)
nrow(data)
ncol(data)
data_train<-data[1:16544,]
data_test<-data[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(e1071)
data_classifier<-naiveBayes(data_train,data_train_labels)
data_pred<-predict(data_classifier,data_test)
a=table(data_pred,data_test_labels)
a
library(gmodels)
CrossTable(data_pred,data_test_labels,prop.chisq = FALSE,prop.t = FALSE,dnn = c('predicted','actual'))
library(caret)
confusionMatrix(a)
# naive bayes on intrusion detection
data<-read.csv(file.choose())
data
str(data)
# got to know that class can be a factor
data$class<-factor(data$class)
nrow(data)
ncol(data)
data_train<-data[1:16544,]
data_test<-data[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(e1071)
data_classifier<-naiveBayes(data_train,data_train_labels)
data_pred<-predict(data_classifier,data_test)
a=table(data_pred,data_test_labels)
a
library(gmodels)
CrossTable(data_pred,data_test_labels,prop.chisq = FALSE,prop.t = FALSE,dnn = c('predicted','actual'))
library(caret)
confusionMatrix(a)
# knn on intrusion dataset
data<-read.csv(file.choose())
data
normalise<-function(x){
return(x-min(x)/max(x)-min(x))
}
data
nrow(data)
ncol(data)
normalise<-function(x){
return(x-min(x)/max(x)-min(x))
}
data_n<-as.data.frame(lapply(data,normalise))
data_n<-as.data.frame(lapply(data[1:42],normalise))
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return(x-min(x)/max(x)-min(x))
}
data_n<-as.data.frame(lapply(data[1:42],normalise))
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[1:42],normalise))
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data[1:16544,]
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
# knn on intrusion dataset
data<-read.csv(file.choose())
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
wbcd_test_prepd<-knn(train = data_train, test = data_test,cl=data_train_labels,k=21)
library(gmodels)
CrossTable(x=data_test_labels,y=data_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
data_test_prepd<-knn(train = data_train, test = data_test,cl=data_train_labels,k=21)
library(gmodels)
CrossTable(x=data_test_labels,y=data_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
aa<-table(data_test_labels,data_test_prepd)
library(caret)
confusionMatrix(aa)  #this tells about the accuracy of the model
# knn on intrusion dataset
data<-read.csv(file.choose())
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
data_test_prepd<-knn(data_train, data_test,data_train_labels,k=21)
library(gmodels)
CrossTable(x=data_test_labels,y=data_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
aa<-table(data_test_labels,data_test_prepd)
library(caret)
confusionMatrix(aa)  #this tells about the accuracy of the model
table(data$class)
# knn on intrusion dataset
data<-read.csv(file.choose())
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
table(data$class)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
data_test_prepd<-knn(data_train, data_test,data_train_labels,k=21)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
data_test_prepd<-knn(data_train, data_test,data_train_labels,k=21)
library(gmodels)
CrossTable(x=data_test_labels,y=data_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
aa<-table(data_test_labels,data_test_prepd)
library(caret)
confusionMatrix(aa)  #this tells about the accuracy of the model
# knn on diamond dataset
library(dplyr)
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
View(data)
colnames(data)
ncol(data)
nrow(data)
str(data)
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
View(data)
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
library(class)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x)/(max(x)-min(x))))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
gc()
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x)/(max(x)-min(x))))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
View(daat_n)
View(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:53940,]
data_train_labels<-data[1:45000,2]
data_test_labels<-data[45001:53940,2]
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=30)
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=25)
gc()
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
View(data)
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:53940,]
data_train_labels<-data[1:45000,2]
data_test_labels<-data[45001:53940,2]
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=25)
# naive bayes algorithm on diamonds dataset
data<-diamonds
# naive bayes algorithm on diamonds dataset
data<-diamonds
nrow(data)
ncol(data)
# naive bayes algorithm on diamonds dataset
data<-diamonds
nrow(data)
ncol(data)
data_train<-data[1:45000,]
data_test<-data[45001:53940,]
data_train_labels<-data[1:45000,]$cut
data_test_labels<-data[45001:53940,]$cut
library(e1071)
data_classifier<-naiveBayes(data_train,data_train_labels)
data_pred<-predict(data_test,data_classifier)
data_pred<-predict(data_classifier,data_test)
a=table(data_pred,data_test_labels)
CrossTable(data_pred,data_test_labels,prop.chisq = FALSE,prop.t = FALSE,dnn = c('predicted','actual'))
confusionMatrix(a)
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
View(data)
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=25)
data_test_prepd<-knn(train=data_train,test=data_test,class=data_train_labels,k=25)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
length(data_train)
length(data_train_labels)
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
data$cut<-as.factor(data$cut)
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data),]
# Reset row names to ensure proper indexing
rownames(data_train) <- NULL
rownames(data_test) <- NULL
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
length(wbcd_train)
wbcd<-read.csv("wisc_bc_data.csv",stringsAsFactors = FALSE)
str(wbcd) #tells the structure of the data
View(wbcd)
wbcd<-wbcd[-1] #removing the first column from the data since the ID is not useful for the anlaysis
table(wbcd$diagnosis)  #gives the count of unique strings
wbcd$diagnosis<-factor(wbcd$diagnosis,levels=c("B","M"),labels=c("Benign","Malignant"))
round(prop.table(table(wbcd$diagnosis))*100,digits = 1)  # prop means proportion. this actually tells the proportion of the values present in the field
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd_n<-as.data.frame(lapply(wbcd[2:31],normalize))
summary(wbcd_n$area_mean)
wbcd_train<-wbcd_n[1:469,]  #using 470 samples to train
wbcd_test<-wbcd_n[470:569,]  #using 100 samples to test
wbcd_train_labels<-wbcd[1:469,1]
wbcd_test_labels<-wbcd[470:569,1]
length(wbcd_train)
length(wbcd_train_labels)
library(class)
wbcd_test_prepd<-knn(train = wbcd_train, test = wbcd_test,cl=wbcd_train_labels,k=21)
library(gmodels)
CrossTable(x=wbcd_test_labels,y=wbcd_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
aa<-table(wbcd_test_labels,wbcd_test_prepd)
library(caret)
confusionMatrix(aa)  #this tells about the accuracy of the model
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
data$cut<-as.factor(data$cut)
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data_n),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
CrossTable(x=data_train_labels,y=data_test_prepd,prop.chisq = FALSE)
View(data)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=5)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=255)
# knn on intrusion dataset
data<-read.csv(file.choose())
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
table(data$class)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
data_test_prepd<-knn(data_train, data_test,data_train_labels,k=21)
library(gmodels)
gc()
gc()
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
data$cut<-as.factor(data$cut)
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data_n),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data_n),]
data_train_labels<-data[1:45000,]$cut
data_test_labels<-data[45001:nrow(data),]$cut
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
CrossTable(x=data_train_labels,y=data_test_prepd,prop.chisq = FALSE)
CrossTable(x = data_test_labels, y = data_test_prepd, prop.chisq = FALSE)
a<-table(data_test_labels,data_test_prepd)
confusionMatrix(a)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=50)
CrossTable(x = data_test_labels, y = data_test_prepd, prop.chisq = FALSE)
a<-table(data_test_labels,data_test_prepd)
confusionMatrix(a)
