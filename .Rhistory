nrow(data)
str(data)
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
View(data)
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
library(class)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x)/(max(x)-min(x))))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
gc()
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x)/(max(x)-min(x))))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
View(daat_n)
View(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:53940,]
data_train_labels<-data[1:45000,2]
data_test_labels<-data[45001:53940,2]
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=30)
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=25)
gc()
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
View(data)
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:53940,]
data_train_labels<-data[1:45000,2]
data_test_labels<-data[45001:53940,2]
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=25)
# naive bayes algorithm on diamonds dataset
data<-diamonds
# naive bayes algorithm on diamonds dataset
data<-diamonds
nrow(data)
ncol(data)
# naive bayes algorithm on diamonds dataset
data<-diamonds
nrow(data)
ncol(data)
data_train<-data[1:45000,]
data_test<-data[45001:53940,]
data_train_labels<-data[1:45000,]$cut
data_test_labels<-data[45001:53940,]$cut
library(e1071)
data_classifier<-naiveBayes(data_train,data_train_labels)
data_pred<-predict(data_test,data_classifier)
data_pred<-predict(data_classifier,data_test)
a=table(data_pred,data_test_labels)
CrossTable(data_pred,data_test_labels,prop.chisq = FALSE,prop.t = FALSE,dnn = c('predicted','actual'))
confusionMatrix(a)
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
View(data)
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
data_test_prepd<-knn(data_train,data_test,data_train_labels,k=25)
data_test_prepd<-knn(train=data_train,test=data_test,class=data_train_labels,k=25)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
length(data_train)
length(data_train_labels)
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
data$cut<-as.factor(data$cut)
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data),]
# Reset row names to ensure proper indexing
rownames(data_train) <- NULL
rownames(data_test) <- NULL
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
length(wbcd_train)
wbcd<-read.csv("wisc_bc_data.csv",stringsAsFactors = FALSE)
str(wbcd) #tells the structure of the data
View(wbcd)
wbcd<-wbcd[-1] #removing the first column from the data since the ID is not useful for the anlaysis
table(wbcd$diagnosis)  #gives the count of unique strings
wbcd$diagnosis<-factor(wbcd$diagnosis,levels=c("B","M"),labels=c("Benign","Malignant"))
round(prop.table(table(wbcd$diagnosis))*100,digits = 1)  # prop means proportion. this actually tells the proportion of the values present in the field
summary(wbcd[c("radius_mean","area_mean","smoothness_mean")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
wbcd_n<-as.data.frame(lapply(wbcd[2:31],normalize))
summary(wbcd_n$area_mean)
wbcd_train<-wbcd_n[1:469,]  #using 470 samples to train
wbcd_test<-wbcd_n[470:569,]  #using 100 samples to test
wbcd_train_labels<-wbcd[1:469,1]
wbcd_test_labels<-wbcd[470:569,1]
length(wbcd_train)
length(wbcd_train_labels)
library(class)
wbcd_test_prepd<-knn(train = wbcd_train, test = wbcd_test,cl=wbcd_train_labels,k=21)
library(gmodels)
CrossTable(x=wbcd_test_labels,y=wbcd_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
aa<-table(wbcd_test_labels,wbcd_test_prepd)
library(caret)
confusionMatrix(aa)  #this tells about the accuracy of the model
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
data$cut<-as.factor(data$cut)
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data_n),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
CrossTable(x=data_train_labels,y=data_test_prepd,prop.chisq = FALSE)
View(data)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=5)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=255)
# knn on intrusion dataset
data<-read.csv(file.choose())
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
table(data$class)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
data_test_prepd<-knn(data_train, data_test,data_train_labels,k=21)
library(gmodels)
gc()
gc()
# knn on diamond dataset
library(class)
library(caret)
library(gmodels)
data<-diamonds
colnames(data)
str(data)
# by analyzing dataset, we can make cut as a factor and it is a factor
data$cut<-as.factor(data$cut)
ncol(data)
nrow(data)
table(data$cut)
round(prop.table(table(data$cut))*100,digits=1)
summary(data[c("depth","table","price","x","y","z")])
normalize<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:10],normalize))
summary(data_n)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data_n),]
data_train_labels<-data[1:45000,'cut']
data_test_labels<-data[45001:nrow(data),'cut']
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
data_train<-data_n[1:45000,]
data_test<-data_n[45001:nrow(data_n),]
data_train_labels<-data[1:45000,]$cut
data_test_labels<-data[45001:nrow(data),]$cut
length(data_train)
length(data_train_labels)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=25)
CrossTable(x=data_train_labels,y=data_test_prepd,prop.chisq = FALSE)
CrossTable(x = data_test_labels, y = data_test_prepd, prop.chisq = FALSE)
a<-table(data_test_labels,data_test_prepd)
confusionMatrix(a)
data_test_prepd<-knn(train=data_train,test=data_test,cl=data_train_labels,k=50)
CrossTable(x = data_test_labels, y = data_test_prepd, prop.chisq = FALSE)
a<-table(data_test_labels,data_test_prepd)
confusionMatrix(a)
# knn on intrusion dataset
data<-read.csv(file.choose())
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
table(data$class)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
data_test_prepd<-knn(data_train, data_test,data_train_labels,k=21)
library(gmodels)
CrossTable(x=data_test_labels,y=data_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
aa<-table(data_test_labels,data_test_prepd)
library(caret)
confusionMatrix(aa)  #this tells about the accuracy of the model
library(rpart)
library(rpart.plot)
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
str(data)
nrow(data)
ncol(data)
set.seed(1234)
indexes<-sample(1:nrow(data),0.70*nrow(data))
data_train<-data[indexes,]
data_test<-data[-indexes,]
target<- class ~ src_bytes +	dst_bytes +	land +	wrong_fragment +	urgent +	hot +	num_failed_logins +	logged_in +	num_compromised +	root_shell +	su_attempted +	num_root +	num_file_creations +	num_shells +	num_access_files +	num_outbound_cmds +	is_host_login +	is_guest_login +	count +	srv_count +	serror_rate +	srv_serror_rate +	rerror_rate +	srv_rerror_rate +	same_srv_rate +	diff_srv_rate +	srv_diff_host_rate +	dst_host_count +	dst_host_srv_count +	dst_host_same_srv_rate +	dst_host_diff_srv_rate +	dst_host_same_src_port_rate +	dst_host_srv_diff_host_rate +	dst_host_serror_rate +	dst_host_srv_serror_rate +	dst_host_rerror_rate +	dst_host_srv_rerror_rate
tree<-rpart(target, data=data_train, method = "class")
rpart.plot(tree)
predictions<-predict(tree,data_test,type="class")
confusion_matrix<-table(data_test$class,predictions)
print(confusion_matrix)
accuracy<-sum(diag(confusion_matrix))/sum(confusion_matrix)
print(paste("Accuracy: ",round(accuracy*100,4)))
# naive bayes on intrusion detection
data<-read.csv(file.choose())
data
View(data)
str(data)
# got to know that class can be a factor
data$class<-factor(data$class)
nrow(data)
ncol(data)
data_train<-data[1:16544,]
data_test<-data[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(e1071)
data_classifier<-naiveBayes(data_train,data_train_labels)
data_pred<-predict(data_classifier,data_test)
a=table(data_pred,data_test_labels)
a
library(gmodels)
CrossTable(data_pred,data_test_labels,prop.chisq = FALSE,prop.t = FALSE,dnn = c('predicted','actual'))
library(caret)
confusionMatrix(a)
# knn on intrusion dataset
data<-read.csv(file.choose())
data
data$class<-factor(data$class)
nrow(data)
ncol(data)
table(data$class)
round(prop.table(table(data$class))*100,digits = 1)
normalise<-function(x){
return((x-min(x))/(max(x)-min(x)))
}
data_n<-as.data.frame(lapply(data[5:41],normalise))
data_train<-data_n[1:16544,]
data_test<-data_n[16545:25192,]
data_train_labels<-data[1:16544,]$class
data_test_labels<-data[16545:25192,]$class
library(class)
data_test_prepd<-knn(data_train, data_test,data_train_labels,k=21)
library(gmodels)
CrossTable(x=data_test_labels,y=data_test_prepd,prop.chisq=FALSE)  # this gives the details about the number of values matched and not matched while testing
aa<-table(data_test_labels,data_test_prepd)
library(caret)
confusionMatrix(aa)  #this tells about the accuracy of the model
# step-1: load the necessary packages
library(rpart)
library(rpart.plot)
# step-2: load the iris dataset
data(iris)
str(iris)
# step-3: split the data into training and testing sets
set.seed(42)  # setting seed for reproducibility
indexes=sample(1:nrow(iris),0.7*nrow(iris))  # randomly selecting the indexes from iris dataset for training and testing
# as we are randomising the data for training and testing, the data will be randomised everytime we run the code, so we need to use set.seed so that the same data will be used which was previously randomised
iris_train<-iris[indexes,]  # training data(70%)
View(iris_train)
iris_test<-iris[-indexes,] # testing data(30%)
# step-4: define the target formula and train the decision tree
target<-Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width  # we are saying that the target/ feature to be classified is Species by comupting the columns given after '~' sign
# tree=rpart(target, data=iris_train, method="class, control=rpart.control(minsplit=4))
tree<-rpart(target, data=iris_train, method="class") #class defines that the tree should be made for classification, not for regression
# step-5: visualize the decision tree
rpart.plot(tree)
# step-6: make predictions on the test set
predictions<-predict(tree,iris_test,type="class")
# step-7: evaluate the model performance by creating a confusion matrix
confusion_matrix<-table(iris_test$Species,predictions)
print(confusion_matrix)
# step-8: calculate the accuracy of the model
accuracy=sum(diag(confusion_matrix))/sum(confusion_matrix)  # the formula used is present in 'caret' library. this calculates the sum of diagonal values present in the confusion matrix divided by sum of all values present in confusion matrix
print(paste("Accuracy: ",round(accuracy*100,4)))
library(rpart)
library(rpart.plot)
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
str(data)
nrow(data)
ncol(data)
set.seed(1234)
indexes<-sample(1:nrow(data),0.70*nrow(data))
data_train<-data[indexes,]
data_test<-data[-indexes,]
target<- class ~ src_bytes +	dst_bytes +	land +	wrong_fragment +	urgent +	hot +	num_failed_logins +	logged_in +	num_compromised +	root_shell +	su_attempted +	num_root +	num_file_creations +	num_shells +	num_access_files +	num_outbound_cmds +	is_host_login +	is_guest_login +	count +	srv_count +	serror_rate +	srv_serror_rate +	rerror_rate +	srv_rerror_rate +	same_srv_rate +	diff_srv_rate +	srv_diff_host_rate +	dst_host_count +	dst_host_srv_count +	dst_host_same_srv_rate +	dst_host_diff_srv_rate +	dst_host_same_src_port_rate +	dst_host_srv_diff_host_rate +	dst_host_serror_rate +	dst_host_srv_serror_rate +	dst_host_rerror_rate +	dst_host_srv_rerror_rate
tree<-rpart(target, data=data_train, method = "class")
rpart.plot(tree)
predictions<-predict(tree,data_test,type="class")
confusion_matrix<-table(data_test$class,predictions)
print(confusion_matrix)
accuracy<-sum(diag(confusion_matrix))/sum(confusion_matrix)
print(paste("Accuracy: ",round(accuracy*100,4)))
# applying linear regression model on iris dataset
data(iris)
head(iris)
model<-lm(Sepal.Length~Petal.Length, data = iris)
summary(model)
plot(iris$Petal.Length,iris$Sepal.Length, main = "Linear Regression on iris data",xlab="Petal Length",ylab="Sepal Length")
# applying linear regression model on iris dataset
data(iris)
head(iris)
model<-lm(Sepal.Length~Petal.Length, data = iris)
summary(model)
plot(iris$Petal.Length,iris$Sepal.Length, main = "Linear Regression on iris data",xlab="Petal Length",ylab="Sepal Length")
plot(iris$Petal.Length,iris$Sepal.Length, main = "Linear Regression on iris data",xlab="Petal Length",ylab="Sepal Length",pch=19,col="blue")
# adding the regression line to the previous chart
abline(model,col="red")
# height predictor vector
x<-c(5.1,5.5,5.8,6.1,6.4,6.7,6.4,6.1,5.10,5.7)
# weight response vector
y<-c(63,66,69,72,75,78,75,72,69,66)
# linear regression model
relation<-lm(y~x)
summary(relation)
# find weight of the person with given height
a<-data.frame(x=6.3)
a
result<-predict(relation,a)
result
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
model<-lm(data$class ~ data$dst_host_count)
summary(model)
model<-lm(data$class ~ data$src_bytes)
data$class<-factor(data$class)
model<-lm(data$class ~ data$src_bytes)
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
data$class<-factor(data$class)
model<-lm(data$class ~ data$src_bytes)
str(data)
data$class<-as.numeric(data$class)
model<-lm(data$class ~ data$src_bytes)
summary(model)
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
str(data)
data$class<-as.numeric(data$class)
model1<-lm(data$class ~ data$duration)
data$class<-factor(data$class)
data$class<-as.numeric(data$class)
model1<-lm(data$class ~ data$duration)
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
str(data)
data$class<-factor(data$class)
data$class<-as.numeric(data$class)
model1<-lm(data$class ~ data$duration)
model2<-lm(data$class ~ data$protocol_type)
summary(model1)
summary(model2)
plot(data$duration,data$class, main = "Linear Regression on intrusion detection",xlab="Duration",ylab="Class",pch=19,col="blue")
plot(data$protocol_type,data$class, main = "Linear Regression on intrusion detection",xlab="Protocol type",ylab="Class",pch=19,col="blue")
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
str(data)
data$class<-factor(data$class)
data$class<-as.numeric(data$class)
model1<-lm(data$class ~ data$duration)
model2<-lm(data$class ~ data$protocol_type)
summary(model1)
plot(data$duration,data$class, main = "Linear Regression on intrusion detection",xlab="Duration",ylab="Class",pch=19,col="blue")
summary(model2)
plot(data$protocol_type,data$class, main = "Linear Regression on intrusion detection",xlab="Protocol type",ylab="Class",pch=19,col="blue")
model2<-lm(data$class ~ data$src_bytes)
summary(model2)
plot(data$protocol_type,data$class, main = "Linear Regression on intrusion detection",xlab="Protocol type",ylab="Class",pch=19,col="blue")
model2<-lm(data$class ~ data$dst_host_serror_rate)
summary(model2)
plot(data$protocol_type,data$class, main = "Linear Regression on intrusion detection",xlab="Protocol type",ylab="Class",pch=19,col="blue")
# linear regression model on intrusion detection dataset
data<-read.csv("C://lpu//5th sem//INT234//Datasets//intrusion_detection_data.csv")
str(data)
data$class<-factor(data$class)
data$class<-as.numeric(data$class)
model1<-lm(data$class ~ data$duration)
model2<-lm(data$class ~ data$protocol_type)
model3<-lm(data$class~data$service)
model4<-lm(data$class~data$flag)
model5<-lm(data$class~data$src_bytes)
summary(model1)
plot(data$duration,data$class, main = "Linear Regression on intrusion detection",xlab="Duration",ylab="Class",pch=19,col="blue")
summary(model2)
summary(model3)
summary(model4)
summary(model5)
abline(model1,col="red")
